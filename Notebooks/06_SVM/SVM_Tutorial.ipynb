{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q71qY1utlDUm"
      },
      "source": [
        "# Support Vector Machine\n",
        "\n",
        "SVMs are algorithms that can be used for both classification and regression purposes, although they are more commonly used for **classification**.\n",
        "\n",
        "To understand how SVM's work, imagine each data point a vector point in $n$-dimensional space, where $n$ is the total number of features.\n",
        "\n",
        "An SVM classifies data by finding the **optimal hyperplane** that best divides the data into groups by class. In a 2D space (data with two features), this \"hyperplane\" is a line dividing the space into two regions, as shown below. The trick to finding the optimal line is maximizing the distance from the line to any data. This is the **maximum margin**.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1200/0*0o8xIA4k3gXUDCFU.png\">\n",
        "\n",
        "\n",
        "**Support vectors** are the data points nearest to the hyperplane. If these points were removed, the optimal hyperplane would change. The position and number of points outside the support vectors will not change the hyperplane fit at all. This means SVM's can have solid results on small datasets (with valuable support vectors). In the image below, the support vectors are circled.\n",
        "\n",
        "![](https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/Images/Week1/svm_4.png))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0-2PaniEWk9",
        "colab_type": "text"
      },
      "source": [
        "Intuitively, the further from the hyperplane our support vectors points lie, the larger the margin, and the more confident we are in our classifier. Therefore, we ideally want our data points to be as far away from the hyperplane as possible, while still being on the correct side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh0EH1TzEWlA",
        "colab_type": "text"
      },
      "source": [
        "So what happens when data overlaps, or doesn't have a clear dividing line? Take this image as an example:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/720/1*fv8DDZLaR0t7SO-W6tdDAg.png\">\n",
        "\n",
        "Here we have these two options. Try to draw a line despite some points being on the wrong side:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/600/1*1dwut8cWQ-39POHV48tv4w.png\">\n",
        "\n",
        "Or give up on having a straight line, and define a curved or segmented line instead:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/600/1*gt_dkcA5p0ZTHjIpq1qnLQ.png\"> \n",
        "\n",
        "Both options can work! However, there are tradeoffs. In some cases, the first option may not be accurate enough. However, the second option may take too long for large data sets, and may over-fit to the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vfvZCdtTla0p"
      },
      "source": [
        "## Important Parameters for SVM\n",
        "\n",
        "In this notebook, we will be using sklearn's SVC (Support Vector Classifier, documentation found here: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). Feel free to look at the other available SVM models here: https://scikit-learn.org/stable/modules/svm.html.\n",
        "\n",
        "In this section we will describe three key parameters for training SVMs: Regularization, Gamma, and Kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fVGb0z6EWlC",
        "colab_type": "text"
      },
      "source": [
        "### Regularization\n",
        "**Regularization (C)** impacts the division of data by telling the SVM optimization how much you want to avoid misclassifying the training data.\n",
        "- Low regularization values create smooth decision boundaries\n",
        "- High regularization values create more complex decision boundaries, but may over-fit to the training set\n",
        "\n",
        "        low C:\n",
        "<img src=\"https://miro.medium.com/max/600/1*1dwut8cWQ-39POHV48tv4w.png\">\n",
        "\n",
        "        high C:\n",
        "<img src=\"https://miro.medium.com/max/600/1*gt_dkcA5p0ZTHjIpq1qnLQ.png\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFzIqLy0EWlE",
        "colab_type": "text"
      },
      "source": [
        "### Gamma\n",
        "**Gamma** defines how close a training data point needs to be to impact the decision boundary. High gamma can lead to a lot of the data not being considered.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/720/1*dGDQxV8j83VB90skHsXktw.png\">\n",
        "<img src=\"https://miro.medium.com/max/720/1*ClmsnU_yb1YtIwAAr7krmg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We-EM7x-EWlF",
        "colab_type": "text"
      },
      "source": [
        "### Kernel\n",
        "A **kernel** is essentially a transformation that makes decision boundaries possible for different shaped distributions. In the example below, it is impossible to draw a straight line to separate the circles from the squares.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/720/1*C3j5m3E3KviEApHKleILZQ.png\"> <br><br>\n",
        "\n",
        "However, if we apply a kernel to transform the data into 3D space (for example, with z = x² + y²), we may be able to draw a line on the Z-X or Z-Y plane.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/720/1*FLolUnVUjqV0EGm3CYBPLw.png\"> <br><br>\n",
        "\n",
        "Looking at it again in X-Y, we have managed to separate the data quite well.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/720/1*NN5VCpVg9gPCLYrDl0YFYw.png\"> <br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0bM5mvsEWlK",
        "colab_type": "text"
      },
      "source": [
        "# Example SVM\n",
        "\n",
        "As an example, we'll use an SVM to predict diabetes using the Pima Diabetes dataset. Load and view the data in the cells below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_GGUYGvEWlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -- imports -- #\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOyMSQyqEWlR",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9-qgexhEWlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -- loading dataset -- #\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "\n",
        "# remember we're trying to predict the datapoint's \"label\" in this case the last column, 'class'\n",
        "    #  'class': '0' means does not have diabetes and '1' means has diabetes\n",
        "\n",
        "data = pd.read_csv(url, names=names)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7emPUZUlEWlX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "5e49f33c-a993-41c7-dedc-4414d7603f43"
      },
      "source": [
        "# -- dropping NaN rows -- #\n",
        "invalid = ['plas', 'pres', 'skin', 'test', 'mass']\n",
        "\n",
        "for i in invalid:\n",
        "    data[i].replace(to_replace=0, value=np.nan, inplace=True)\n",
        "    \n",
        "data = data.dropna(axis=0).reset_index(drop=True)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>test</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>78.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.248</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>197.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>543.0</td>\n",
              "      <td>30.5</td>\n",
              "      <td>0.158</td>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>189.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>846.0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.398</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   preg   plas  pres  skin   test  mass   pedi  age  class\n",
              "0     1   89.0  66.0  23.0   94.0  28.1  0.167   21      0\n",
              "1     0  137.0  40.0  35.0  168.0  43.1  2.288   33      1\n",
              "2     3   78.0  50.0  32.0   88.0  31.0  0.248   26      1\n",
              "3     2  197.0  70.0  45.0  543.0  30.5  0.158   53      1\n",
              "4     1  189.0  60.0  23.0  846.0  30.1  0.398   59      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00EMh2hBEWlc",
        "colab_type": "text"
      },
      "source": [
        "## Splitting Training/Validation/Testing\n",
        "\n",
        "Now let's clearly define which columns will act as explanatory variables, and which column will be the target value. Then, we will split the dataset between your training data (80%) and testing data (20%). We will use sklearn's train_test_split method, which is documented here:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "We will set random_state to 0 so we get the same output each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBtdMz_vEWld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# columns we will use to make predictions with (features!) feel free to play around with these\n",
        "X_cols = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
        "\n",
        "# column that we want to predict\n",
        "y_col = 'class'\n",
        "\n",
        "# 80-20 train-test split of datset\n",
        "test_size = 0.2\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[X_cols], data[y_col], test_size=test_size, random_state=0)\n",
        "\n",
        "# further split X and y of training nto training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBR277yFEWli",
        "colab_type": "text"
      },
      "source": [
        "## Building the Model\n",
        "Next, we create a model using SVC, and fit the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW-Ac5pQEWll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "dad5e52b-84a1-40a2-cd7e-7cbdb6d96564"
      },
      "source": [
        "# creating a model with sklearn's SVC\n",
        "svm = SVC(gamma=.1, C=1)\n",
        "\n",
        "# training/fitting a model with training data\n",
        "svm.fit(X_train, y_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DnG36VMEWlq",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPkcLqS_EWlr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f59b6a20-761b-4af7-eba5-fd3044534526"
      },
      "source": [
        "#printing accuracy of testing & training data\n",
        "y_train_pred=svm.predict(X_train)\n",
        "print(\"Training Accuracy is \", accuracy_score(y_train, y_train_pred)*100)\n",
        "y_val_pred=svm.predict(X_val)\n",
        "print(\"Validation Accuracy is \", accuracy_score(y_val,y_val_pred)*100)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy is  100.0\n",
            "Validation Accuracy is  63.49206349206349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POLOa6NAEWlu",
        "colab_type": "text"
      },
      "source": [
        "As you can see above, despite achieving a training accuracy of 100%, the validation accuracy is only 63%. This suggests that the model has been **over-fit**! \n",
        "\n",
        ">In general if your training accuracy ever reaches 100%, you've over-fit your model\n",
        "\n",
        "Play around with the parameters to try to balance out the accuracies. You can start with the ones we've mentioned above, but look through documentation for more options!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjb0uy-jEWlv",
        "colab_type": "text"
      },
      "source": [
        "Once you feel like your model's at a good place, you can do one last evaluation using the **testing data**. Don't forget, your testing data should never be used to change your model and is reserved for one last evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmyJg3jvEWlw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "c5cb3fa0-5f2f-4c36-cd2e-9e9f90e8fe21"
      },
      "source": [
        "y_test_pred=svm.predict(X_test)\n",
        "\n",
        "print(\"Training Accuracy is \", accuracy_score(y_train, y_train_pred)*100)\n",
        "print(\"Validation Accuracy is \", accuracy_score(y_val,y_val_pred)*100)\n",
        "print(\"Testing Accuracy is \", accuracy_score(y_test,y_test_pred)*100)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy is  100.0\n",
            "Validation Accuracy is  63.49206349206349\n",
            "Testing Accuracy is  68.35443037974683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmfXxVxgEWl0",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "Pros of SVM\n",
        "- SVM works well with small data sets with many attributes\n",
        "- SVM models run fast and don't use much memory, as they only depend on a few support vectors\n",
        "\n",
        "Cons of SVM\n",
        "- Training time is long, which isn't well suited for larger data sets\n",
        "- SVM is less effective on \"noisier\" datasets with overlapping classes\n",
        "- Results are very dependent on parameters, which can be hard to tune on small data sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOiqnNAbEWl2",
        "colab_type": "text"
      },
      "source": [
        "## Resource\n",
        "\n",
        "Lesson adapted from https://medium.com/machine-learning-101\n",
        "> This is really great series of articles on introductory machine learning, take a look if you feel like you need additional clarification"
      ]
    }
  ]
}