{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xb0q8y0rq6Ve"
   },
   "source": [
    "# Nearest Neighbors Algorithms\n",
    "\n",
    "The basic concept for **Nearest Neighbors Algorithms** (NNAs) is to classify a datum by finding one or more points that have similar features. The most similar points are called the **nearest neighbors**. Once found, the input datum can be put in the same class as its neighbors. We can also use this to predict the value of missing features for the input datum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdwUJygSGU9F"
   },
   "source": [
    "## k-Nearest Neighbors\n",
    "\n",
    "The most commonly used NNA is **k-Nearest Neighbors,** in which the top $k$ nearest neighbors (best matches) are identified. In most instantiations of k-NNA, classification or prediction is based on a **majority vote** of the $k$ nearest neighbors.\n",
    ">For example, if $k = 5$, and at least 3 out of the 5 nearest neighbors to an input datum are class A, then we would assign the new datum to class A.\n",
    "\n",
    "For a more complex example, see the image below. Here, if $k = 1$, the green circle would be assigned to Class 1, since the nearest point is a blue square. However, if $k = 3$ the answer becomes Class 2, since the next two closest are both red triangles.\n",
    "\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/0*Sk18h9op6uK9EpT8.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZrldzQgy9ud"
   },
   "source": [
    "## Quick example to illustrate a kNN:\n",
    "\n",
    "In this section, we will use sklearn to build a k-NNA model for a data set describing cars. Our goal will be to classify the cars into one of two categories: \"cool\" or \"uncool.\" Clearly these are subjective terms, but that's ok: we will provide a manually classified training set.\n",
    "\n",
    "For example, if we consider the variables *horsepower, number of seats,* and *manual (0) or automatic (1)*, our manually classified training set might look like this:\n",
    "\n",
    "*   150, 5, 0, uncool (2008 Honda Civic)  \n",
    "*   320, 5, 0, cool (2011 Dodge Charger)\n",
    "*   383, 3, 1, cool (1985 Chevy Blazer)\n",
    "*   210, 7, 0, uncool (2001 Honda Odyssey)\n",
    "\n",
    "Let's say we're trying to predict whether the 2017 Bugatti Veyron (1500hp, 2 seats, manual: 1) is cool or not. Our first step is to load the data into a python structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "s4VMsygjy9ue",
    "outputId": "e45a264a-29e9-43d4-d322-78c76841ae03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp</th>\n",
       "      <th>seats</th>\n",
       "      <th>auto</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985 Chevy Blazer</th>\n",
       "      <td>383.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001 Honda Odyssey</th>\n",
       "      <td>210.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008 Honda Civic</th>\n",
       "      <td>150.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011 Dodge Charger</th>\n",
       "      <td>320.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017 Bugatti Veyron</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         hp  seats  auto  cool\n",
       "1985 Chevy Blazer     383.0    3.0   1.0   1.0\n",
       "2001 Honda Odyssey    210.0    7.0   0.0   0.0\n",
       "2008 Honda Civic      150.0    5.0   0.0   0.0\n",
       "2011 Dodge Charger    320.0    5.0   0.0   1.0\n",
       "2017 Bugatti Veyron  1500.0    2.0   1.0   NaN"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- imports -- #\n",
    "import numpy as np\n",
    "import pandas as po\n",
    "\n",
    "# -- loading the data -- #\n",
    "cars_dict = {'2008 Honda Civic':    {'hp':150., 'seats':5., 'auto':0., 'cool':0}, \n",
    "             '2011 Dodge Charger' : {'hp':320., 'seats':5., 'auto':0., 'cool':1}, \n",
    "             '1985 Chevy Blazer':   {'hp':383., 'seats':3., 'auto':1., 'cool':1}, \n",
    "             '2001 Honda Odyssey':  {'hp':210., 'seats':7., 'auto':0., 'cool':0}, \n",
    "             '2017 Bugatti Veyron': {'hp':1500.,'seats':2., 'auto':1., 'cool':None}}\n",
    "\n",
    "data = po.DataFrame.from_dict(cars_dict,orient='index')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5STDuS1DF0PA"
   },
   "source": [
    "### Normalizing Data and Calculating Distance\n",
    "\n",
    "To determine the class of our Bugatti using NNA, we need to measure its \"nearness\" to other cars. One of the simplest metrics for this is Euclidian distance, defined as the square root of the sum of the difference between each feature squared:\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/2y0bx.png.)\n",
    "\n",
    "Some other approaches include Chi square distance and cosine distance. For further reading on distance functions, see this article: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/.\n",
    "\n",
    "Note that **by using Euclidian distance, we are normalizing the data in a way that assumes all features are equally important**. One consequence of this is that values that are generally larger (such as horsepower) will end up having a larger impact on the distance. Unless we have some reason to believe that the larger features are more important, we will want to balance features by **normalizing the data.**\n",
    "\n",
    "\n",
    "One quick and easy way to normalize data is to divide each datum by the maximum value in its category. Let's do that for our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "aBhWCPsFMn-C",
    "outputId": "48fe2f59-9e6d-4613-b3a0-3bbf9f6466c3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp</th>\n",
       "      <th>seats</th>\n",
       "      <th>auto</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985 Chevy Blazer</th>\n",
       "      <td>0.255333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001 Honda Odyssey</th>\n",
       "      <td>0.140000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008 Honda Civic</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011 Dodge Charger</th>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017 Bugatti Veyron</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           hp     seats  auto  cool\n",
       "1985 Chevy Blazer    0.255333  0.428571   1.0   1.0\n",
       "2001 Honda Odyssey   0.140000  1.000000   0.0   0.0\n",
       "2008 Honda Civic     0.100000  0.714286   0.0   0.0\n",
       "2011 Dodge Charger   0.213333  0.714286   0.0   1.0\n",
       "2017 Bugatti Veyron  1.000000  0.285714   1.0   NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing the data by dividing each value by the maximum value in its row. \n",
    "# Remember that we don't normalize the label ()\n",
    "\n",
    "data_norm_divide = data\n",
    "\n",
    "for i in ['hp','seats','auto']:\n",
    "    data_norm_divide[i] = data[i]/max(data[i].values)\n",
    "    \n",
    "data_norm_divide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXcfmnfzy9u0"
   },
   "source": [
    "As usual, a tool already exists to do this. Using sklearn, we can normalize data with an object called a StandardScaler. We will use this in the example below, but feel free to also read the documentation here: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html.\n",
    "\n",
    "Normalization, or more generally \"standardization,\" is a common requirement for machine learning estimators. Beyond shifting values to similar magnitudes, many estimators will run into trouble if the values within individual features are not roughly normally distributed. By **normal distribution,** we mean a Gaussian distribution with 0 mean and unit variance. The sklearn StandardScaler will attempt to scale all the data to approximate a normal distribution for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "V6hK0zn1y9u2",
    "outputId": "9c6c556c-2a9d-454f-8e63-5f97180560e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp</th>\n",
       "      <th>seats</th>\n",
       "      <th>auto</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985 Chevy Blazer</th>\n",
       "      <td>-0.259004</td>\n",
       "      <td>-0.802955</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001 Honda Odyssey</th>\n",
       "      <td>-0.604742</td>\n",
       "      <td>1.491202</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008 Honda Civic</th>\n",
       "      <td>-0.724651</td>\n",
       "      <td>0.344124</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011 Dodge Charger</th>\n",
       "      <td>-0.384908</td>\n",
       "      <td>0.344124</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017 Bugatti Veyron</th>\n",
       "      <td>1.973305</td>\n",
       "      <td>-1.376494</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           hp     seats      auto  cool\n",
       "1985 Chevy Blazer   -0.259004 -0.802955  1.224745   1.0\n",
       "2001 Honda Odyssey  -0.604742  1.491202 -0.816497   0.0\n",
       "2008 Honda Civic    -0.724651  0.344124 -0.816497   0.0\n",
       "2011 Dodge Charger  -0.384908  0.344124 -0.816497   1.0\n",
       "2017 Bugatti Veyron  1.973305 -1.376494  1.224745   NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize data by removing the mean and scaling to unit variance from each feature\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_unitnorm = po.DataFrame.from_dict(cars_dict,orient='index')\n",
    "\n",
    "for i in ['hp','seats','auto']:\n",
    "    feature_data = data_unitnorm[i].values.reshape(-1, 1)\n",
    "    scaler.fit(feature_data)\n",
    "    data_unitnorm[i] = scaler.transform(feature_data)\n",
    "    \n",
    "data_unitnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6dLhGxay9u7"
   },
   "source": [
    "Now that the data is standardized, we can begin building our NNA algorithm. First, we will need a function that **calculates the Euclidean distance between two data points**. For our purposes, we will assume the data points will be stored in arrays of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rayoFQnOyvY"
   },
   "outputs": [],
   "source": [
    "# Distance function using formula for euclidean distance\n",
    "\n",
    "def euclidean_dist(datum1, datum2):\n",
    "    inner_val = 0.0\n",
    "    \n",
    "    for g in range(datum1.shape[0]):\n",
    "        inner_val += (datum1[g]- datum2[g]) ** 2\n",
    "    \n",
    "    distance = math.sqrt(inner_val)\n",
    "    return(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IaktG5_mUNf"
   },
   "source": [
    "Next, we calculate the distance between our Bugatti and each other car using the *euclidean_dist* function. For the sake of testing, we do this twice: once for the normalized data (*data_norm_divide*), and once for the data standardized by sklearn (*data_unitnorm*). Note that we will not input the classification column of cool/uncool to our distance function, which means taking a subset of each array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "rY0Wuzz9LZzu",
    "outputId": "15bbbefb-d5e7-4ff4-92f6-645e58f8336e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distances to 2017 Bugatti Veyron (V1)\n",
      "  0: \t0.758\n",
      "  1: \t1.500\n",
      "  2: \t1.412\n",
      "  3: \t1.343\n",
      "  4: \t0.000\n",
      "\n",
      "Euclidean Distances to 2017 Bugatti Veyron (V2)\n",
      "  0: \t2.305\n",
      "  1: \t4.363\n",
      "  2: \t3.796\n",
      "  3: \t3.562\n",
      "  4: \t0.000\n"
     ]
    }
   ],
   "source": [
    "# FYI: This is how you can call a specific row by name and sub-select features\n",
    "bugatti = data.loc[\"2017 Bugatti Veyron\"][[\"hp\",\"seats\",\"auto\"]].values\n",
    "\n",
    "import math\n",
    "\n",
    "# -- normalized data by dividing -- #\n",
    "print('Euclidean Distances to 2017 Bugatti Veyron (V1)')\n",
    "for car in range(len(data_norm_divide)):\n",
    "    d = euclidean_dist(data_norm_divide.iloc[4,:3].values, \n",
    "                       data_norm_divide.iloc[car, :3].values)\n",
    "    print('  {}: \\t{:01.3f}'.format(car,d))\n",
    "\n",
    "# -- standardized data -- #\n",
    "print('\\nEuclidean Distances to 2017 Bugatti Veyron (V2)')\n",
    "for car in range(len(data_unitnorm)):\n",
    "    d = euclidean_dist(data_unitnorm.iloc[4,:3].values, \n",
    "                       data_unitnorm.iloc[car, :3].values)\n",
    "    print('  {}: \\t{:01.3f}'.format(car,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHRn4t68nIbB"
   },
   "source": [
    "For the normalized data (with each feature divided by its maximum value), we get the following distances:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Blazer = 0.758\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Odyssey = 1.500\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Civic = 1.412\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Charger = 1.343\n",
    "\n",
    "For the standardized data (using sklearn's StandardScaler), we get the following distances:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Blazer = 2.305\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Odyssey = 4.363\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Civic = 3.796\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Charger = 3.562\n",
    "\n",
    "Notice that both techniques yielded the same order of cars from nearest to farthest.\n",
    ">This is coincidental, and unlikely to happen with larger or more varied data sets.\n",
    "\n",
    "Since the distance between the Bugatti and Blazer is the smallest, if $k = 1$, we would classify the Bugatti as cool. However, if $k = 4$ there would no longer be a majority, and we would not be able to classify the Bugatti in either category without a tiebreaker protocol.\n",
    "\n",
    "Generally speaking, *larger values of $k$ reduce noise, but also make the boundaries between classes less distinct*. The best value of $k$ will vary by data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vo5iwH6soijh"
   },
   "source": [
    "# Example kNN \n",
    "\n",
    "Next we will see if we can use k-NNA on the Pima diabetes dataset.\n",
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "hQAjk52WQ1da",
    "outputId": "14b27fa3-9a09-4762-aa2e-bcb576400d6c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>197.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>189.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg   plas  pres  skin   test  mass   pedi  age  class\n",
       "0     1   89.0  66.0  23.0   94.0  28.1  0.167   21      0\n",
       "1     0  137.0  40.0  35.0  168.0  43.1  2.288   33      1\n",
       "2     3   78.0  50.0  32.0   88.0  31.0  0.248   26      1\n",
       "3     2  197.0  70.0  45.0  543.0  30.5  0.158   53      1\n",
       "4     1  189.0  60.0  23.0  846.0  30.1  0.398   59      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- loading dataset -- #\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = po.read_csv(url, names=names)\n",
    "\n",
    "# -- dropping NaN rows -- #\n",
    "invalid = ['plas', 'pres', 'skin', 'test', 'mass']\n",
    "\n",
    "for i in invalid:\n",
    "    data[i].replace(to_replace=0, value=np.nan, inplace=True)\n",
    "    \n",
    "data = data.dropna(axis=0).reset_index(drop=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lO_knzAAe93n"
   },
   "source": [
    "Now, let's clearly define which columns will act as explanatory variables, and which column will be the target value, and split the dataset between your training data and testing data.  Let's try an 80-20 split and use sklearn's [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method (set random_state = 0 so we get the same output each time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "b8pRTCC3PatJ",
    "outputId": "47ceb8c1-e308-4974-ba9f-91587232c917"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# columns we will use to make predictions with (features!) feel free to play around with these\n",
    "X_cols = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
    "\n",
    "# column that we want to predict\n",
    "y_col = 'class'\n",
    "\n",
    "# 80-20 train-test split of datset\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[X_cols], data[y_col], test_size=test_size, random_state=0)\n",
    "\n",
    "# further split X and y of training nto training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrnVJ8Ovy9vT"
   },
   "source": [
    "## Normalizing Data\n",
    "\n",
    "Let's not forget to normalize the data! We'll use sklearn's StandardScaler normalization like we did before to normalize the training **and** testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lt29VKMJy9vT",
    "outputId": "900e8d17-5e40-4ce4-840d-c4aa910893e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "for i in list(X_train):\n",
    "    feature_data_train = X_train[i].values.reshape(-1, 1)\n",
    "    scaler.fit(feature_data_train)\n",
    "    X_train[i] = scaler.transform(feature_data_train)\n",
    "\n",
    "for j in list(X_test):\n",
    "    feature_data_test = X_test[j].values.reshape(-1, 1)\n",
    "    scaler.fit(feature_data_test)\n",
    "    X_test[j] = scaler.transform(feature_data_test)\n",
    "    \n",
    "for k in list(X_val):\n",
    "    feature_data_val = X_val[k].values.reshape(-1, 1)\n",
    "    scaler.fit(feature_data_val)\n",
    "    X_val[k] = scaler.transform(feature_data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-2ZM8AGIe0S"
   },
   "source": [
    "## Using sklearn's k-NNA\n",
    "\n",
    "Luckily for us, sklearn has some quick and easy functions for normalizing the data, finding Euclidean distances, training, and testing with [k-NNA](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). Try k = 5 to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 13073
    },
    "colab_type": "code",
    "id": "R0FsKkp_qw-x",
    "outputId": "39c27885-10d9-4f03-9bff-ac7c41d5dcb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# creating model with sklearn's KNeighborsClassifier -- after running these cells play around with the parameter n!\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# training/fitting a model with training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J8y1RuAoy9vr",
    "outputId": "e203abae-7fcc-4314-d277-b4febb322b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.002027750015258789 seconds\n",
      "Training Accuracy is  79.2\n",
      "Validation Accuracy is  80.95238095238095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "y_train_pred=knn.predict(X_train)\n",
    "start = time.time()\n",
    "predictions_fast = knn.predict(X_val)\n",
    "\n",
    "print('Took {} seconds'.format(time.time() - start))\n",
    "print(\"Training Accuracy is \", accuracy_score(y_train, y_train_pred)*100)\n",
    "print(\"Validation Accuracy is \", accuracy_score(y_val,predictions_fast)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-hHcKgsy9vw"
   },
   "source": [
    "# Conclusions and Limitations\n",
    "\n",
    "k-NNA provides a good baseline classifier for machine learning that is conceptually intuitive and easy to implement and train. One major advantage is that, by standardizing input data, k-NNA can combine any number of features regardless of their original distributions.\n",
    "\n",
    "However, k-NNA can run into problems with more complex data sets:\n",
    "* With additional dimensions, it can be harder to define meaningful distances.\n",
    "* The testing phase can slow down significantly with large data sets since each point's distance is measured to every other point.\n",
    "* \"Majority votes\" may be skewed if one classification is significantly more common than the others.\n",
    "* There is no consideration for correlated features.\n",
    "\n",
    "As a final note, there are other ways to instantiate k-NNA's, and other types of NNA beyond k-Nearest. For example, to solve the large majority problem, we could have used a weighted voting system where nearer neighbors' votes carry more weight."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Nearest_Neighbors_Solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
